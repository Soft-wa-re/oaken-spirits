{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from json import loads\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                         value_serializer=lambda x: \n",
    "                         dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP = 1\n",
    "\n",
    "def make_serializable(d):\n",
    "    serializable_dict = {}\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, (list, dict, str, int, float, bool, type(None))):\n",
    "            serializable_dict[key] = value\n",
    "        else:\n",
    "            serializable_dict[key] = str(value)  # Convert non-serializable value to string\n",
    "    return serializable_dict\n",
    "\n",
    "def publish_to_kafka(file_path, producer=producer, chunk_size=1000):\n",
    "    \"\"\" \n",
    "    Send each row to kafka mysql topic\n",
    "    \n",
    "    Uncomment either the environment code if using .env file or \n",
    "    variable code to set directly.\n",
    "    \n",
    "    SLEEP variable can be adjusted as desired. 1 is the minimum recommended setting. \n",
    "    Setting to 0 will crash the Kafka server unless a more robust EC2 setup is created.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        while True:\n",
    "            chunk = []\n",
    "            for _ in range(chunk_size):\n",
    "                try:\n",
    "                    chunk.append(next(reader))\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            if not chunk:\n",
    "                break\n",
    "            for row in chunk:\n",
    "                # Convert row to JSON\n",
    "                row = make_serializable(row)\n",
    "                json_data = row\n",
    "\n",
    "                # Publish row to Kafka\n",
    "                producer.send('mysql', value=json_data)\n",
    "                producer.flush()\n",
    "                sleep(SLEEP)\n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpublish_to_kafka\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/Iowa_Liquor_Sales.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36mpublish_to_kafka\u001b[0;34m(file_path, producer, chunk_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m             producer\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmysql\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39mjson_data)\n\u001b[1;32m     40\u001b[0m             producer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m---> 41\u001b[0m             sleep(SLEEP)\n\u001b[1;32m     42\u001b[0m producer\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "publish_to_kafka(\"data/Iowa_Liquor_Sales.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Topics\n",
    "\n",
    "Kafka Docker\n",
    "\n",
    "1. `docker exec -it kafka1 bash`\n",
    "1. `cd /usr/bin`\n",
    "1. `kafka-topics --list --bootstrap-server localhost:9092`\n",
    "    - Edit the script below for desired topic\n",
    "1. `kafka-console-consumer --bootstrap-server localhost:9092 --topic mysql --from-beginning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually flush data when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "\n",
    "# Create a consumer instance\n",
    "consumer = KafkaConsumer(\n",
    "    'mysql',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',  # Start consuming from the earliest offset\n",
    "    enable_auto_commit=True,       # Automatically commit offsets\n",
    "    group_id='my_consumer_group',  # Specify a consumer group\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "# Poll for messages\n",
    "try:\n",
    "    for message in consumer:\n",
    "        # Print the received message value\n",
    "        print(\"Received message:\", message.value)\n",
    "\n",
    "# Handle KeyboardInterrupt to gracefully close the consumer\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer terminated by user\")\n",
    "\n",
    "# Close the consumer\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "from json import loads\n",
    "import random\n",
    "import datetime\n",
    "import mysql.connector\n",
    "import boto3\n",
    "from logging.handlers import RotatingFileHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_to_kafka(\"data/Iowa_Liquor_Sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Poll for messages\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka2/lib/python3.11/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka2/lib/python3.11/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka2/lib/python3.11/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka2/lib/python3.11/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka2/lib/python3.11/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka2/lib/python3.11/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka2/lib/python3.11/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m~/miniconda3/envs/kafka2/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mysql_conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='mysql',\n",
    "    password='mysql',\n",
    "    database='oaken'\n",
    ")\n",
    "mysql_cursor = mysql_conn.cursor()\n",
    "\n",
    "# Create a consumer instance\n",
    "consumer = KafkaConsumer(\n",
    "    'mysql',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',  # Start consuming from the earliest offset\n",
    "    enable_auto_commit=True,       # Automatically commit offsets\n",
    "    group_id='my_consumer_group',  # Specify a consumer group\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "consumer.subscribe(topics=['mysql'])\n",
    "\n",
    "# Poll for messages\n",
    "try:\n",
    "    for message in consumer:\n",
    "        try:\n",
    "            data = message.value\n",
    "            # Customer\n",
    "            storNumber = int(data.get('StoreNumber', ''))\n",
    "            if not storNumber:\n",
    "                logger.error(\"StoreNumber is null or invalid. Skipping insertion.\")\n",
    "                continue\n",
    "\n",
    "            storeName = data.get('StoreName', '')\n",
    "            address = data.get('Address', '')\n",
    "            city = data.get('City', '')\n",
    "            county = data.get('County', '')\n",
    "            state = data.get('State', '')\n",
    "            zip_code = int(data.get('ZipCode', ''))\n",
    "\n",
    "            # vendor\n",
    "            vendorNumber = int(float(data.get('VendorNumber', '')))\n",
    "            if not vendorNumber:\n",
    "                logger.error(\"VendorNumber is null or invalid. Skipping insertion.\")\n",
    "                continue\n",
    "\n",
    "            vendorName = data.get('VendorName', '')\n",
    "\n",
    "            # category\n",
    "            category = int(float(data.get('Category','')))\n",
    "            if not category:\n",
    "                logger.error(\"Category is null or invalid. Skipping insertion.\")\n",
    "                continue\n",
    "\n",
    "            categoryName = (data.get('CategoryName',''))\n",
    "\n",
    "            # product\n",
    "            itemNumber = int(data.get('ItemNumber', ''))\n",
    "            if not itemNumber:\n",
    "                logger.error(\"ItemNumber is null or invalid. Skipping insertion.\")\n",
    "                continue\n",
    "            itemDescription = data.get('ItemDescription', '')\n",
    "            pack = int(data.get('Pack', ''))\n",
    "            volume = int(data.get('BottleVolumeML', ''))\n",
    "            cost = float(data.get('BottleCost', '').replace('$', ''))\n",
    "            retail = float(data.get('BottleRetail', '').replace('$', ''))\n",
    "\n",
    "            # Sales\n",
    "            invoice = data.get('Invoice', '')\n",
    "            if not invoice:\n",
    "                logger.error(\"Invoice is null or invalid. Skipping insertion.\")\n",
    "                continue\n",
    "\n",
    "            date_string = data.get('Date', '')\n",
    "            if not date_string:\n",
    "                logger.error(\"Date is null or invalid. Skipping insertion.\")\n",
    "                continue\n",
    "\n",
    "            sales_date = datetime.datetime.strptime(date_string, '%m/%d/%Y').date()\n",
    "\n",
    "            amountSold = int(data.get('BottlesSold', ''))\n",
    "            totalLiters = float(data.get('VolumeSoldLiters', ''))\n",
    "\n",
    "            sales = float(data.get('SaleDollars', '').replace('$', ''))\n",
    "\n",
    "            # MySQL\n",
    "            '''\n",
    "            Multiple try/except blocks are used due to the simplistic invoicing\n",
    "            application script.\n",
    "\n",
    "            In a real world scenario, likely each of these blocks would be done\n",
    "            as a truly separate process.\n",
    "\n",
    "            The try/except prevents the duplicates failing to be added to the\n",
    "            database from preventing the rest of the processes from competing.\n",
    "            '''\n",
    "\n",
    "            try:\n",
    "                CUSTOMER_QUERY = '''\n",
    "                    INSERT INTO customer (StoreNumber,StoreName,Address,City,CountyName,State,ZipCode)\n",
    "                    VALUES (%s,%s,%s,%s,%s,%s,%s)\n",
    "                    '''\n",
    "                customer_data = (storNumber,storeName,address,city,county,state,zip_code)\n",
    "                mysql_cursor.execute(CUSTOMER_QUERY, customer_data)\n",
    "                mysql_conn.commit()\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                VENDOR_QUERY = '''\n",
    "                    INSERT INTO vendor (VendorNumber,VendorName)\n",
    "                    VALUES (%s,%s)\n",
    "                    '''\n",
    "                vendor_data = (vendorNumber,vendorName)\n",
    "                mysql_cursor.execute(VENDOR_QUERY,vendor_data)\n",
    "                mysql_conn.commit()\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                CATEGORY_QUERY = '''\n",
    "                    INSERT INTO category (CategoryNumber,CategoryName)\n",
    "                    VALUES (%s,%s)\n",
    "                    '''\n",
    "                category_data = (category,categoryName)\n",
    "                mysql_cursor.execute(CATEGORY_QUERY, category_data)\n",
    "                mysql_conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing message: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                PRODUCT_QUERY = '''\n",
    "                    INSERT INTO product (ItemNumber,CategoryNumber,ItemDescription,BottleVolumeML,\n",
    "                    Pack,BottleCost,BottleRetail)\n",
    "                    VALUES (%s,%s,%s,%s,%s,%s,%s,)\n",
    "                    '''\n",
    "                product_data = (itemNumber,category,itemDescription,volume,pack,cost,retail)\n",
    "                mysql_cursor.execute(PRODUCT_QUERY, product_data)\n",
    "                mysql_conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing message: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                SALES_QUERY = '''\n",
    "                INSERT INTO sales (Invoice,StoreNumber,VendorNumber,SaleDate,SaleDollars,\n",
    "                ItemNumber,VolumeSoldLiters,BottlesSold)\n",
    "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                '''\n",
    "                sales_data = (invoice,storNumber,vendorNumber,sales_date,sales,itemNumber,\n",
    "                                totalLiters,amountSold)\n",
    "                mysql_cursor.execute(SALES_QUERY, sales_data)\n",
    "                mysql_conn.commit()\n",
    "\n",
    "                # Topic should post after MySQL processing to ensure data is in the database.\n",
    "                INVOICES_info = {\n",
    "                    'Invoice': invoice,\n",
    "                    'SaleDate': sales_date,\n",
    "                    'saleDollars': sales\n",
    "                }\n",
    "\n",
    "                invoice_producer.send(INVOICES_TOPIC, value=INVOICES_info)\n",
    "                invoice_producer.flush()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing message: {e}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Kafka consumer: {e}\", exc_info=True)\n",
    "            continue\n",
    "\n",
    "# Close the consumer\n",
    "finally:\n",
    "    consumer.close()\n",
    "    mysql_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/Iowa_Liquor_Sales.csv\", nrows=10)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample output\n",
    "'''\n",
    "{'Invoice/Item Number': 'S29198800001', 'Date': '11/20/2015', 'Store Number': '2191', 'Store Name': 'Keokuk Spirits', \n",
    "'Address': '1013 MAIN', 'City': 'KEOKUK', 'Zip Code': '52632', 'Store Location': '1013 MAIN\\nKEOKUK 52632\\n(40.39978, -91.387531)', \n",
    "'County Number': '56', 'County': 'Lee', 'Category': '', 'Category Name': '', 'Vendor Number': '255', 'Vendor Name': 'Wilson Daniels Ltd.', \n",
    "'Item Number': '297', 'Item Description': 'Templeton Rye w/Flask', 'Pack': '6', 'Bottle Volume (ml)': '750', 'State Bottle Cost': '$18.09', \n",
    "'State Bottle Retail': '$27.14', 'Bottles Sold': '6', 'Sale (Dollars)': '$162.84', 'Volume Sold (Liters)': '4.50', 'Volume Sold (Gallons)': '1.19'}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Iowa_Liquor_Sales.csv\", 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    while True:\n",
    "        chunk = []\n",
    "        for _ in range(1000):\n",
    "            try:\n",
    "                chunk.append(next(reader))\n",
    "            except StopIteration:\n",
    "                break\n",
    "        if not chunk:\n",
    "            break\n",
    "        for row in chunk:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# money conversion test\n",
    "data = {'SaleDollars': '$12.34'}\n",
    "\n",
    "sales = float(data.get('SaleDollars','').replace('$', ''))\n",
    "\n",
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime test\n",
    "from datetime import datetime\n",
    "\n",
    "date_data = {\"Date\":\"11/09/2015\"}\n",
    "\n",
    "date_string = date_data.get('Date', '')\n",
    "invoice_date = datetime.strptime(date_string,'%m/%d/%Y').date()\n",
    "\n",
    "print(invoice_date)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
